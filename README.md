# EEP-CoT-SelfAsk Fact Verification

This project evaluates different prompt strategies (EEP, CoT, and Self-Ask) for factual consistency in large language models, based on FEVER and PubHealth datasets.

## ğŸ“¦ How to Use

1. **Download** all files and folders in the root directory (2 folders and one environment config file `.env`).
2. **Manually open** the following markdown files and download the Google Drive content inside. Place the downloaded files as instructed:
   - `FEVER/Fever_Data/Fever.db.md` (**Required**)
   - `FEVER/Fever_Data/train.jsonl(Optional).md` *(Optional)*
   - `Pubhealth/Pubhealth_Data/train.tsv(Optional).md` *(Optional)*
3. **Insert your OpenAI API key** into the `.env` file.
4. **Run** your target experiment script (files beginning with `run_experiment...`).
5. **Note**: OpenAI APIs may be rate-limited. It is recommended to run the scripts in intervals. The program automatically resumes from saved batches (default `batch_size = 500`, adjustable in the script).
6. **Use** `combine.py` to merge output files and generate `combine.csv`.
7. **Use** `static.py` to analyze `combine.csv` and produce visual reports.

## ğŸ“ Structure

```
EEP-CoT-SelfAsk/
â”œâ”€â”€ FEVER/
â”‚   â””â”€â”€ Fever_Data/
â”‚       â”œâ”€â”€ Fever.db.md
â”‚       â””â”€â”€ train.jsonl(Optional).md
â”œâ”€â”€ Pubhealth/
â”‚   â””â”€â”€ Pubhealth_Data/
â”‚       â””â”€â”€ train.tsv(Optional).md
â”œâ”€â”€ .env
â”œâ”€â”€ run_experiment_*.py
â”œâ”€â”€ combine.py
â””â”€â”€ static.py
```

## ğŸ§  Notes

- Ensure all dependencies are installed.
- Keep all files relative to the root for correct path resolution.